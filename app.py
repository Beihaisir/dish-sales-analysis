# app.py (V7.3 - æŒ‡å®šâ€œä¸€å¤©å†…æ—¶æ®µçª—å£â€å æ¯”é¥¼å›¾ + å¢å¼ºåˆ†ç±»è§„åˆ™)
# ä½ è¦çš„é€»è¾‘ï¼šåœ¨ä½ ã€æŸ¥è¯¢ã€‘é€‰å®šçš„æ—¥æœŸèŒƒå›´å†…ï¼Œå†é€‰ä¸€ä¸ªâ€œä¸€å¤©å†…æ—¶æ®µçª—å£â€ï¼ˆå¦‚ 11:30-14:30ï¼‰ï¼Œ
# è®¡ç®—ï¼šçª—å£å†…é”€é‡ / æŸ¥è¯¢èŒƒå›´æ€»é”€é‡ï¼Œå¹¶ç”¨é¥¼å›¾å±•ç¤ºï¼ˆçª—å£å†… vs çª—å£å¤–ï¼‰
import re
import io
import json
import hashlib
import datetime as dt
from typing import Tuple, Dict, List

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px


# =========================
# åŸºç¡€é…ç½®
# =========================
st.set_page_config(page_title="èœå“é”€å”®åˆ†æï¼ˆæ­£å¤§é¤é¥®ï¼‰", layout="wide")
st.title("èœå“é”€å”®åˆ†æ")

# =========================
# é»˜è®¤åˆ†ç±»æ˜ å°„ï¼ˆå·²åˆå¹¶æ–°å¢æœªå‘½ä¸­èœå“çš„åˆ†ç±»å»ºè®®ï¼‰
# =========================
DEFAULT_CATEGORY_MAP = {
    "æ¿ç­‹ç±»": ["æ¿ç­‹"],
    "çŒªè‚ç±»": ["çŒªè‚"],
    "é¸¡ä¸ç±»": ["é¸¡ä¸"],
    "å¤è›‹ç±»": ["å¤è›‹", "å¤é¸¡è›‹"],
    "ç…è›‹ç±»": ["ç…è›‹"],

    # âœ… é¥®æ–™ç±»ï¼ˆæ‰©å……ï¼‰
    "é¥®æ–™ç±»": [
        "é¥®æ–™", "å¯ä¹", "é›ªç¢§", "ç¾å¹´è¾¾", "å†œå¤«å±±æ³‰",
        "åŒ—å†°æ´‹", "çº¢ç‰›", "åŠ å¤šå®", "å”¯æ€¡è±†å¥¶", "æ¤°æ±", "è‹¹æœé†‹",
        "æ©™æ±", "èŠ’æœæ±", "æ¨æ¢…", "å°é’æŸ ", "å°è’™ç‰›",
        "çŸ¿æ³‰æ°´", "æ­£å¤§çŸ¿æ³‰æ°´", "åŒ—å…­æŸ ç™¾ç¾",
        "èŠ±ç”Ÿå‘³6ç‚¹åŠ", "ç»ç’ƒç“¶", "å¬è£…", "ç”Ÿæ¦¨", "é¥®å“", "é¥­ååŠå°æ—¶"
    ],

    "ç‰›è‚‰ç±»": ["ç‰›è‚‰"],
    "è…°èŠ±ç±»": ["è…°èŠ±"],
    "é¸¡èƒ—ç±»": ["é¸¡èƒ—"],
    "è‚¥è‚ ç±»": ["è‚¥è‚ "],
    "é¸¡æ‚ç±»": ["é¸¡æ‚"],
    "åŒè„†ç±»": ["åŒè„†"],
    "å³é£Ÿç±»": ["ä¸‰é²œ", "è€æ¯é¸¡"],

    # âœ… æ–°å¢åˆ†ç±»
    "å¤å‘³ç±»": ["å¤è±†è…", "å¤é¸¡è…¿", "å¤çŒªè„š", "å¤é¸¡è„š", "å°å¤æ‹¼", "ç‰¹è‰²å°å¤æ‹¼"],
    "é³é±¼ç±»": ["é³é±¼"],
    "æŒä¸­å®ç±»": ["æŒä¸­å®"],
    "è€ä¸‰ä¸ç±»": ["è€ä¸‰ä¸"],
    "äº”èŠ±è‚‰ç±»": ["äº”èŠ±è‚‰"],
    "ä¸»é£Ÿç‚¹å¿ƒç±»": ["çº¢ç³–é¦’å¤´", "é²œè‚‰åŒ…", "æµæ²™åŒ…", "å°ç¬¼åŒ…", "è’¸é¥º", "çƒ§éº¦"],
    "é±¼é¦™è‚‰ä¸ç±»": ["é±¼é¦™è‚‰ä¸"],

    "å…¶ä»–ç±»": ["æ‰“åŒ…ç›’", "å•ä»½ç±³é¥­", "åŠ é¢", "é¸¡æ’", "å‡‘ä»·"],
}

# è›‹ç±»å›ºå®šå•ä»·ï¼ˆè¥ä¸šé¢ä¿®æ­£ï¼‰
FIXED_PRICE_CATEGORY = {"å¤è›‹ç±»": 2.0, "ç…è›‹ç±»": 2.0}
EGG_CATEGORIES = set(FIXED_PRICE_CATEGORY.keys())

# åšæ³•å•åŠ ç™½åå•ï¼ˆå‡ºç°ä¸€æ¬¡è®°ä¸€æ¬¡ Ã— èœå“æ•°é‡ï¼‰
ADDON_WHITELIST = [
    "åŠ é¸¡ä¸", "åŠ ç‰›è‚‰", "åŠ æ¿ç­‹", "åŠ è…°èŠ±",
    "åŠ çŒªè‚", "åŠ é¸¡èƒ—", "åŠ è‚¥è‚ ", "åŠ é¸¡æ‚",
    "æ‰“åŒ…"
]


# =========================
# å·¥å…·å‡½æ•°
# =========================
def sha1_text(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()[:10]

def normalize_str(x) -> str:
    if pd.isna(x):
        return ""
    return str(x).strip()

def make_topn_with_others(df: pd.DataFrame, name_col: str, value_col: str, topn: int = 20) -> pd.DataFrame:
    df = df.sort_values(value_col, ascending=False).copy()
    if len(df) <= topn:
        return df
    top = df.head(topn).copy()
    others_sum = df.iloc[topn:][value_col].sum()
    others = pd.DataFrame({name_col: ["å…¶ä»–"], value_col: [others_sum]})
    return pd.concat([top, others], ignore_index=True)

def parse_rule_table(rule_df: pd.DataFrame) -> Dict[str, List[str]]:
    """
    è§„åˆ™è¡¨è¦æ±‚è‡³å°‘ä¸¤åˆ—ï¼šåˆ†ç±», å…³é”®è¯ï¼ˆä¸­æ–‡åˆ—åéœ€ä¸¥æ ¼ä¸€è‡´ï¼‰
    æ”¯æŒä¸€ä¸ªåˆ†ç±»å¯¹åº”å¤šå…³é”®è¯ï¼ˆå¤šè¡Œï¼‰
    """
    cols = [c.strip() for c in rule_df.columns.astype(str)]
    rule_df.columns = cols

    if "åˆ†ç±»" not in rule_df.columns or "å…³é”®è¯" not in rule_df.columns:
        raise ValueError("è§„åˆ™è¡¨å¿…é¡»åŒ…å«åˆ—ï¼šåˆ†ç±»ã€å…³é”®è¯ï¼ˆä¸¥æ ¼ä¸­æ–‡åˆ—åï¼‰")

    rule_df = rule_df[["åˆ†ç±»", "å…³é”®è¯"]].copy()
    rule_df["åˆ†ç±»"] = rule_df["åˆ†ç±»"].map(normalize_str)
    rule_df["å…³é”®è¯"] = rule_df["å…³é”®è¯"].map(normalize_str)
    rule_df = rule_df[(rule_df["åˆ†ç±»"] != "") & (rule_df["å…³é”®è¯"] != "")]

    out: Dict[str, List[str]] = {}
    for cat, g in rule_df.groupby("åˆ†ç±»"):
        kws = sorted(set(g["å…³é”®è¯"].tolist()))
        out[cat] = kws

    if not out:
        raise ValueError("è§„åˆ™è¡¨è§£æåä¸ºç©ºï¼Œè¯·æ£€æŸ¥å†…å®¹ã€‚")
    return out

def build_time_bucket(df: pd.DataFrame, minutes: int) -> pd.Series:
    t = pd.to_datetime(df["åˆ›å»ºæ—¶é—´"], errors="coerce")
    return t.dt.floor(f"{minutes}T")

@st.cache_data(show_spinner=False)
def read_excel_safely(file_bytes: bytes) -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    preview = pd.read_excel(bio, sheet_name=0, header=None, nrows=40)

    header_row = None
    for i in range(len(preview)):
        row = preview.iloc[i].astype(str).tolist()
        if ("åˆ›å»ºæ—¶é—´" in row) and ("èœå“åç§°" in row) and ("èœå“æ•°é‡" in row):
            header_row = i
            break
    if header_row is None:
        raise ValueError("æœªæ‰¾åˆ°è¡¨å¤´è¡Œï¼ˆéœ€è¦åŒ…å«ï¼šåˆ›å»ºæ—¶é—´/èœå“åç§°/èœå“æ•°é‡ï¼‰ã€‚")

    bio.seek(0)
    df = pd.read_excel(bio, sheet_name=0, header=header_row)

    needed = ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "èœå“æ•°é‡", "è§„æ ¼åç§°", "åšæ³•", "ä¼˜æƒ åå°è®¡ä»·æ ¼"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—ï¼š{missing}")

    return df[needed].copy()

@st.cache_data(show_spinner=False)
def read_csv_safely_generic(file_bytes: bytes, encoding_guess: str = "utf-8") -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    try:
        df = pd.read_csv(bio, encoding=encoding_guess)
    except Exception:
        bio.seek(0)
        df = pd.read_csv(bio, encoding="gbk")
    return df

@st.cache_data(show_spinner=False)
def read_data_file(file_name: str, file_bytes: bytes) -> pd.DataFrame:
    if file_name.lower().endswith(".csv"):
        df = read_csv_safely_generic(file_bytes)
    else:
        df = read_excel_safely(file_bytes)

    needed = ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "èœå“æ•°é‡", "è§„æ ¼åç§°", "åšæ³•", "ä¼˜æƒ åå°è®¡ä»·æ ¼"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—ï¼š{missing}")
    return df[needed].copy()

def compress_types(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["åˆ›å»ºæ—¶é—´"] = pd.to_datetime(df["åˆ›å»ºæ—¶é—´"], errors="coerce")
    df["èœå“æ•°é‡"] = pd.to_numeric(df["èœå“æ•°é‡"], errors="coerce").fillna(0).astype("int32")
    df["ä¼˜æƒ åå°è®¡ä»·æ ¼"] = pd.to_numeric(df["ä¼˜æƒ åå°è®¡ä»·æ ¼"], errors="coerce").fillna(0).astype("float64")
    for c in ["è§„æ ¼åç§°", "èœå“åç§°", "åšæ³•"]:
        df[c] = df[c].astype(str).fillna("")
        df[c] = df[c].astype("string")
    return df

def build_category_long_df_and_coverage(df: pd.DataFrame, category_map: dict) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    å¤šåˆ†ç±»å±•å¼€ï¼ˆåŒè®¡æ•°ï¼‰
    - åˆ†ç±»åŸºç¡€ï¼šæŒ‰å…³é”®è¯åŒ¹é…å¾—åˆ°çš„åˆ†ç±»
    - åˆ†ç±»ï¼ˆå±•ç¤ºåˆ†ç±»ï¼‰ï¼š
        * èœå“åç§°ä»¥â€œåŠ â€å¼€å¤´ => å•åŠ -<èœå“åç§°>ï¼ˆæ›´ç›´è§‚ï¼Œä¸æ··å…¥å¤§ç±»ï¼‰
        * å¦åˆ™ => åˆ†ç±»åŸºç¡€
    """
    names = df["èœå“åç§°"].astype(str)

    idx_list, cat_list = [], []
    any_hit_mask = np.zeros(len(df), dtype=bool)

    for cat, keywords in category_map.items():
        if not keywords:
            continue
        pattern = "(" + "|".join(re.escape(k) for k in keywords) + ")"
        mask = names.str.contains(pattern, regex=True, na=False).to_numpy()

        any_hit_mask |= mask
        hit_idx = np.flatnonzero(mask)
        if hit_idx.size == 0:
            continue
        idx_list.append(hit_idx)
        cat_list.append(np.repeat(cat, hit_idx.size))

    if idx_list:
        all_idx = np.concatenate(idx_list)
        all_cat = np.concatenate(cat_list)
        long_df = df.iloc[all_idx].copy()
        long_df["åˆ†ç±»åŸºç¡€"] = all_cat
    else:
        long_df = df.iloc[0:0].assign(åˆ†ç±»åŸºç¡€=pd.Series(dtype="object"))

    unmatched = df.loc[~any_hit_mask, ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "èœå“æ•°é‡", "ä¼˜æƒ åå°è®¡ä»·æ ¼", "è§„æ ¼åç§°", "åšæ³•"]].copy()

    coverage = pd.DataFrame({
        "æ€»è¡Œæ•°": [len(df)],
        "å‘½ä¸­è¡Œæ•°": [int(any_hit_mask.sum())],
        "æœªå‘½ä¸­è¡Œæ•°": [int((~any_hit_mask).sum())],
        "å‘½ä¸­ç‡": [float(any_hit_mask.mean()) if len(df) else 0.0]
    })

    dish_name = long_df["èœå“åç§°"].astype(str).str.strip()
    long_df["æ˜¯å¦åŠ èœå“"] = dish_name.str.startswith("åŠ ")
    long_df["åˆ†ç±»"] = np.where(long_df["æ˜¯å¦åŠ èœå“"], "å•åŠ -" + dish_name, long_df["åˆ†ç±»åŸºç¡€"])

    return long_df, unmatched, coverage

def compute_addon_summary_vectorized(cat_df: pd.DataFrame, addon_list: list) -> pd.DataFrame:
    """åšæ³•å•åŠ ï¼šå‡ºç°æ¬¡æ•° Ã— èœå“æ•°é‡ï¼›æŒ‰ã€å±•ç¤ºåˆ†ç±»ã€‘æ±‡æ€»"""
    if cat_df.empty:
        return pd.DataFrame(columns=["åˆ†ç±»", "å•åŠ é¡¹", "æ•°é‡"])

    method = cat_df["åšæ³•"].astype(str).fillna("")
    qty = cat_df["èœå“æ•°é‡"].astype("int64")

    parts = []
    for addon in addon_list:
        counts = method.str.count(re.escape(addon)).astype("int64")
        contrib = counts * qty
        s = contrib.groupby(cat_df["åˆ†ç±»"]).sum()
        tmp = s.rename("æ•°é‡").reset_index()
        tmp["å•åŠ é¡¹"] = addon
        tmp = tmp[["åˆ†ç±»", "å•åŠ é¡¹", "æ•°é‡"]]
        parts.append(tmp)

    out = pd.concat(parts, ignore_index=True)
    out = out[out["æ•°é‡"] > 0].sort_values("æ•°é‡", ascending=False)
    return out

def rule_health_check(df_q: pd.DataFrame, category_map: dict, unmatched_df: pd.DataFrame, cat_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    unmatched_top = (
        unmatched_df.groupby("èœå“åç§°", as_index=False)
        .agg(æœªå‘½ä¸­è¡Œæ•°=("èœå“åç§°", "size"),
             æ•°é‡åˆè®¡=("èœå“æ•°é‡", "sum"),
             å°è®¡åˆè®¡=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"))
        .sort_values(["æ•°é‡åˆè®¡", "å°è®¡åˆè®¡"], ascending=False)
        .head(50)
    )

    multi_hit = (
        cat_df.groupby(["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "èœå“æ•°é‡", "è§„æ ¼åç§°", "åšæ³•", "ä¼˜æƒ åå°è®¡ä»·æ ¼", "æ—¶é—´æ®µ"], as_index=False)["åˆ†ç±»åŸºç¡€"]
        .nunique()
        .rename(columns={"åˆ†ç±»åŸºç¡€": "å‘½ä¸­åˆ†ç±»æ•°"})
    )
    multi_hit_top = multi_hit.sort_values(["å‘½ä¸­åˆ†ç±»æ•°", "èœå“æ•°é‡"], ascending=[False, False]).head(50)

    names = df_q["èœå“åç§°"].astype(str)
    rows = []
    for cat, kws in category_map.items():
        for kw in kws:
            mask = names.str.contains(re.escape(kw), na=False)
            hit_dishes = df_q.loc[mask, "èœå“åç§°"].astype(str).nunique()
            hit_rows = int(mask.sum())
            if hit_rows > 0:
                rows.append({"åˆ†ç±»": cat, "å…³é”®è¯": kw, "å‘½ä¸­èœå“æ•°": int(hit_dishes), "å‘½ä¸­è¡Œæ•°": hit_rows})
    kw_wide = pd.DataFrame(rows)
    if not kw_wide.empty:
        kw_wide = kw_wide.sort_values(["å‘½ä¸­èœå“æ•°", "å‘½ä¸­è¡Œæ•°"], ascending=False).head(100)
    else:
        kw_wide = pd.DataFrame(columns=["åˆ†ç±»", "å…³é”®è¯", "å‘½ä¸­èœå“æ•°", "å‘½ä¸­è¡Œæ•°"])

    return unmatched_top, multi_hit_top, kw_wide


# =========================
# 1) ä¸Šä¼ æ•°æ®æ–‡ä»¶
# =========================
st.subheader("1) ä¸Šä¼ æ•°æ®æ–‡ä»¶")
data_file = st.file_uploader("é€‰æ‹©è®¢å•æ˜ç»†æ–‡ä»¶ï¼ˆExcel æˆ– CSVï¼‰", type=["xlsx", "xls", "csv"], key="data_file")
if data_file is None:
    st.stop()

with st.spinner("è¯»å–æ•°æ®..."):
    raw_bytes = data_file.getvalue()
    df = read_data_file(data_file.name, raw_bytes)

df = compress_types(df)

# =========================
# 2) ä¸Šä¼ è§„åˆ™æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
# =========================
st.subheader("2) åˆ†ç±»è§„åˆ™ï¼ˆå¯é€‰ï¼šä¸Šä¼ è§„åˆ™è¡¨ Excel/CSVï¼‰")
rule_file = st.file_uploader("ä¸Šä¼ åˆ†ç±»è§„åˆ™è¡¨ï¼ˆä¸¤åˆ—ï¼šåˆ†ç±»ã€å…³é”®è¯ï¼‰", type=["xlsx", "xls", "csv"], key="rule_file")

rule_source = "é»˜è®¤è§„åˆ™ï¼ˆå†…ç½®ï¼‰"
rule_updated_at = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

if rule_file is not None:
    with st.spinner("è¯»å–è§„åˆ™è¡¨..."):
        b = rule_file.getvalue()
        if rule_file.name.lower().endswith(".csv"):
            rule_df = read_csv_safely_generic(b)
        else:
            rule_df = pd.read_excel(io.BytesIO(b), sheet_name=0)

        CATEGORY_MAP = parse_rule_table(rule_df)
        rule_source = f"ä¸Šä¼ è§„åˆ™ï¼š{rule_file.name}"
else:
    CATEGORY_MAP = DEFAULT_CATEGORY_MAP

rule_fingerprint = sha1_text(json.dumps(CATEGORY_MAP, ensure_ascii=False, sort_keys=True))

# =========================
# Sidebar é…ç½®
# =========================
with st.sidebar:
    st.header("é…ç½®")
    grain = st.selectbox("æ—¶é—´é¢—ç²’ï¼ˆæœ€å°30åˆ†é’Ÿï¼‰", ["30åˆ†é’Ÿ", "60åˆ†é’Ÿ", "120åˆ†é’Ÿ"], index=0)
    grain_min = {"30åˆ†é’Ÿ": 30, "60åˆ†é’Ÿ": 60, "120åˆ†é’Ÿ": 120}[grain]

    st.subheader("è§„åˆ™ä¿¡æ¯")
    st.write(f"- æ¥æºï¼š{rule_source}")
    st.write(f"- æŒ‡çº¹ï¼š`{rule_fingerprint}`")
    st.write(f"- ç”Ÿæˆæ—¶é—´ï¼š{rule_updated_at}")

    st.subheader("åšæ³•å•åŠ ç™½åå•")
    st.write("ã€".join(ADDON_WHITELIST))

# =========================
# 3) æŸ¥è¯¢åŒºï¼ˆå…ˆé€‰æ—¶é—´å†ç‚¹æŸ¥è¯¢ï¼‰
# =========================
st.subheader("3) æŸ¥è¯¢æ¡ä»¶ï¼ˆå…ˆé€‰æ—¶é—´èŒƒå›´ï¼Œå†ç‚¹å‡»æŸ¥è¯¢ï¼‰")

min_dt = pd.to_datetime(df["åˆ›å»ºæ—¶é—´"], errors="coerce").min()
max_dt = pd.to_datetime(df["åˆ›å»ºæ—¶é—´"], errors="coerce").max()
if pd.isna(min_dt) or pd.isna(max_dt):
    min_dt = pd.Timestamp.today().normalize()
    max_dt = min_dt

if "last_query" not in st.session_state:
    st.session_state.last_query = None
if "queried" not in st.session_state:
    st.session_state.queried = False

col1, colmid, col2, col3 = st.columns([3, 0.5, 3, 1.5])
with col1:
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=min_dt.date())
    start_time = st.time_input("å¼€å§‹æ—¶é—´", value=dt.time(0, 0))
with colmid:
    st.markdown("<div style='text-align:center; font-size:28px; padding-top:28px;'>~</div>", unsafe_allow_html=True)
with col2:
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=max_dt.date())
    end_time = st.time_input("ç»“æŸæ—¶é—´", value=dt.time(23, 59))
with col3:
    st.write(""); st.write("")
    do_query = st.button("ğŸ” æŸ¥è¯¢", type="primary")

start_dt = pd.Timestamp.combine(start_date, start_time)
end_dt = pd.Timestamp.combine(end_date, end_time)
if start_dt > end_dt:
    st.error("å¼€å§‹æ—¶é—´ä¸èƒ½æ™šäºç»“æŸæ—¶é—´ã€‚")
    st.stop()

if do_query:
    st.session_state.last_query = (start_dt, end_dt, grain_min, rule_fingerprint)
    st.session_state.queried = True

if not st.session_state.queried:
    st.info("è¯·å…ˆé€‰æ‹©ä¸‹å•æ—¶é—´èŒƒå›´ï¼Œç„¶åç‚¹å‡»ã€æŸ¥è¯¢ã€‘å¼€å§‹åˆ†æã€‚")
    st.stop()

current_sig = (start_dt, end_dt, grain_min, rule_fingerprint)
if st.session_state.last_query != current_sig:
    st.warning("ä½ å·²ä¿®æ”¹äº†æŸ¥è¯¢æ¡ä»¶/é…ç½®ï¼Œä½†å°šæœªç‚¹å‡»ã€æŸ¥è¯¢ã€‘ï¼›å½“å‰ç»“æœä»æ˜¯ä¸Šä¸€æ¬¡æŸ¥è¯¢çš„ç»“æœã€‚")

q_start_dt, q_end_dt, q_grain_min, _ = st.session_state.last_query

df_q = df[(df["åˆ›å»ºæ—¶é—´"] >= q_start_dt) & (df["åˆ›å»ºæ—¶é—´"] <= q_end_dt)].copy()
if df_q.empty:
    st.warning("è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®ï¼Œè¯·è°ƒæ•´æ—¶é—´èŒƒå›´åå†æŸ¥è¯¢ã€‚")
    st.stop()

df_q["æ—¶é—´æ®µ"] = build_time_bucket(df_q, q_grain_min)

# =========================
# åˆ†ç±»å±•å¼€ + è¦†ç›–ç‡
# =========================
cat_df, unmatched_df, coverage_df = build_category_long_df_and_coverage(df_q, CATEGORY_MAP)
if cat_df.empty:
    st.warning("å½“å‰åˆ†ç±»å…³é”®è¯æœªå‘½ä¸­ä»»ä½•èœå“åç§°ï¼Œè¯·å…ˆå®Œå–„åˆ†ç±»å…³é”®è¯é…ç½®ã€‚")
    st.stop()

# å£å¾„ï¼šé”€é‡è´¡çŒ® & è¥ä¸šé¢è´¡çŒ®
cat_df["é”€é‡è´¡çŒ®"] = cat_df["èœå“æ•°é‡"].astype("int64")
cat_df["è¥ä¸šé¢è´¡çŒ®"] = cat_df["ä¼˜æƒ åå°è®¡ä»·æ ¼"].astype("float64")

# è›‹ç±»ï¼ˆä¾æ®åˆ†ç±»åŸºç¡€ï¼‰ï¼šè¥ä¸šé¢=2Ã—æ•°é‡ï¼›é”€é‡=æ•°é‡ï¼ˆå‘½ä¸­å³é€ä¸€ä¸ªï¼‰
egg_mask = cat_df["åˆ†ç±»åŸºç¡€"].isin(EGG_CATEGORIES)
if egg_mask.any():
    cat_df.loc[egg_mask, "è¥ä¸šé¢è´¡çŒ®"] = (
        cat_df.loc[egg_mask, "åˆ†ç±»åŸºç¡€"].map(FIXED_PRICE_CATEGORY).astype("float64")
        * cat_df.loc[egg_mask, "é”€é‡è´¡çŒ®"].astype("float64")
    )

# KPI
total_sales = int(cat_df["é”€é‡è´¡çŒ®"].sum())
total_rev = float(cat_df["è¥ä¸šé¢è´¡çŒ®"].sum())
num_categories = int(cat_df["åˆ†ç±»"].nunique())
unmatched_rows = int(coverage_df["æœªå‘½ä¸­è¡Œæ•°"].iloc[0])
hit_rate = float(coverage_df["å‘½ä¸­ç‡"].iloc[0])

k1, k2, k3, k4, k5 = st.columns(5)
k1.metric("æ€»é”€é‡ï¼ˆåˆ†ç±»åŒè®¡æ•°ï¼‰", f"{total_sales:,}")
k2.metric("æ€»è¥ä¸šé¢ï¼ˆå«è›‹ç±»ä¿®æ­£ï¼‰", f"{total_rev:,.2f}")
k3.metric("å±•ç¤ºåˆ†ç±»æ•°ï¼ˆå«å•åŠ -ï¼‰", f"{num_categories}")
k4.metric("æœªå‘½ä¸­è¡Œæ•°", f"{unmatched_rows:,}")
k5.metric("å‘½ä¸­ç‡", f"{hit_rate*100:.2f}%")

st.success(f"âœ… å·²ç­›é€‰ï¼š{q_start_dt} ~ {q_end_dt}ï¼›åŸå§‹è¡Œæ•° {len(df_q):,}ï¼›åˆ†ç±»å±•å¼€å {len(cat_df):,}ï¼ˆåŒè®¡æ•°ï¼‰")

# =========================
# ç»Ÿè®¡ï¼šæ—¶æ®µÃ—åˆ†ç±» é”€é‡/å æ¯”
# =========================
qty_time = (
    cat_df.groupby(["æ—¶é—´æ®µ", "åˆ†ç±»"], as_index=False)["é”€é‡è´¡çŒ®"].sum()
    .rename(columns={"é”€é‡è´¡çŒ®": "é”€é‡"})
)
qty_time["å æ¯”"] = qty_time["é”€é‡"] / qty_time.groupby("æ—¶é—´æ®µ")["é”€é‡"].transform("sum")

# å„åˆ†ç±»æ€»é”€é‡ï¼ˆä¸åˆ†è§„æ ¼ï¼‰ + å æ¯”
qty_cat_total = (
    cat_df.groupby("åˆ†ç±»", as_index=False)["é”€é‡è´¡çŒ®"].sum()
    .rename(columns={"é”€é‡è´¡çŒ®": "æ€»é”€é‡"})
    .sort_values("æ€»é”€é‡", ascending=False)
)
qty_cat_total["å æ¯”"] = qty_cat_total["æ€»é”€é‡"] / qty_cat_total["æ€»é”€é‡"].sum()

# åˆ†ç±»Ã—è§„æ ¼ é”€é‡/å æ¯”
qty_spec = (
    cat_df.groupby(["åˆ†ç±»", "è§„æ ¼åç§°"], as_index=False)["é”€é‡è´¡çŒ®"].sum()
    .rename(columns={"é”€é‡è´¡çŒ®": "é”€é‡"})
)
qty_spec["å æ¯”"] = qty_spec["é”€é‡"] / qty_spec.groupby("åˆ†ç±»")["é”€é‡"].transform("sum")

# è§„æ ¼æ€»è®¡é”€é‡ï¼ˆè·¨åˆ†ç±»ï¼‰ï¼šæ ‡å‡†æ‹†ä¸¤æ¡¶ï¼ˆèœå“æ ‡å‡† vs å…¶ä»–æ ‡å‡†ï¼‰
spec_name = cat_df["è§„æ ¼åç§°"].astype(str).str.strip()
is_standard = spec_name.eq("æ ‡å‡†")
is_food_standard = (
    is_standard
    & (~cat_df["åˆ†ç±»"].astype(str).str.startswith("å•åŠ -"))
    & (cat_df["åˆ†ç±»åŸºç¡€"].astype(str).ne("å…¶ä»–ç±»"))
)
cat_df["è§„æ ¼å±•ç¤º"] = np.where(
    is_standard,
    np.where(is_food_standard, "èœå“æ ‡å‡†", "å…¶ä»–æ ‡å‡†"),
    spec_name
)

spec_total_full = (
    cat_df.groupby("è§„æ ¼å±•ç¤º", as_index=False)["é”€é‡è´¡çŒ®"].sum()
    .rename(columns={"è§„æ ¼å±•ç¤º": "è§„æ ¼åç§°", "é”€é‡è´¡çŒ®": "æ€»é”€é‡"})
    .sort_values("æ€»é”€é‡", ascending=False)
)
spec_total_full["å æ¯”"] = spec_total_full["æ€»é”€é‡"] / spec_total_full["æ€»é”€é‡"].sum()

# åˆ†ç±»è¥ä¸šé¢ï¼ˆæŒ‰å±•ç¤ºåˆ†ç±»ï¼‰
rev_cat = (
    cat_df.groupby("åˆ†ç±»", as_index=False)["è¥ä¸šé¢è´¡çŒ®"].sum()
    .rename(columns={"è¥ä¸šé¢è´¡çŒ®": "è¥ä¸šé¢"})
    .sort_values("è¥ä¸šé¢", ascending=False)
)

# åšæ³•å•åŠ ï¼ˆå‘é‡åŒ–ï¼‰
addon_summary = compute_addon_summary_vectorized(cat_df, ADDON_WHITELIST)

# æ‰“åŒ…ç›’æ•°é‡ï¼šæ‰“åŒ…ç›’(èœå“) + æ‰“åŒ…(åšæ³•)
box_item_qty = df_q.loc[df_q["èœå“åç§°"].astype(str).str.contains("æ‰“åŒ…ç›’", na=False), "èœå“æ•°é‡"].sum()
box_item_qty = int(box_item_qty) if pd.notna(box_item_qty) else 0
method_pack_qty = int(addon_summary.loc[addon_summary["å•åŠ é¡¹"] == "æ‰“åŒ…", "æ•°é‡"].sum()) if not addon_summary.empty else 0
packaging_total_qty = box_item_qty + method_pack_qty
packaging_df = pd.DataFrame([
    {"é¡¹": "æ‰“åŒ…ç›’ï¼ˆèœå“åç§°å«æ‰“åŒ…ç›’ï¼‰æ•°é‡", "æ•°é‡": box_item_qty},
    {"é¡¹": "æ‰“åŒ…ï¼ˆåšæ³•å‡ºç°æ¬¡æ•°Ã—èœå“æ•°é‡ï¼‰æ•°é‡", "æ•°é‡": method_pack_qty},
    {"é¡¹": "æ‰“åŒ…ç›’åˆè®¡æ•°é‡ï¼ˆä¸¤è€…ç›¸åŠ ï¼‰", "æ•°é‡": packaging_total_qty},
])

# =========================
# 4) å¯è§†åŒ–åˆ†æ
# =========================
st.divider()
st.header("4) å¯è§†åŒ–åˆ†æ")

# âœ… ä½ è¦çš„ï¼šæŒ‡å®šâ€œä¸€å¤©å†…æ—¶æ®µçª—å£â€é”€é‡å æ¯”ï¼ˆé¥¼å›¾ï¼‰
st.subheader(f"æ‰€é€‰æ—¶é—´èŒƒå›´å†…ï¼šæŒ‡å®šæ—¶æ®µçª—å£é”€é‡å æ¯”ï¼ˆ{grain}ï¼Œä»»æ„å¤©æ•°é€‚ç”¨ï¼‰")

wcol1, wcol2, wcol3 = st.columns([1.2, 1.2, 2])
with wcol1:
    window_start = st.time_input("çª—å£å¼€å§‹ï¼ˆä¸€å¤©å†…ï¼‰", value=dt.time(11, 30), key="window_start")
with wcol2:
    window_end = st.time_input("çª—å£ç»“æŸï¼ˆä¸€å¤©å†…ï¼‰", value=dt.time(14, 30), key="window_end")
with wcol3:
    st.caption("å£å¾„ï¼šåœ¨ä½ å·²ã€æŸ¥è¯¢ã€‘çš„æ—¥æœŸèŒƒå›´å†…ï¼Œç»Ÿè®¡è½åœ¨è¯¥çª—å£å†…çš„é”€é‡å æ€»é”€é‡çš„æ¯”ä¾‹ï¼ˆçª—å£å†… vs çª—å£å¤–ï¼‰ã€‚")

range_total = float(cat_df["é”€é‡è´¡çŒ®"].sum())
if range_total <= 0:
    st.warning("å½“å‰æŸ¥è¯¢èŒƒå›´å†…æ€»é”€é‡ä¸º0ï¼Œæ— æ³•è®¡ç®—å æ¯”ã€‚")
else:
    tod = pd.to_datetime(cat_df["æ—¶é—´æ®µ"]).dt.time

    if window_start <= window_end:
        in_window = (tod >= window_start) & (tod < window_end)
        window_label = f"{window_start.strftime('%H:%M')}â€“{window_end.strftime('%H:%M')}"
    else:
        in_window = (tod >= window_start) | (tod < window_end)
        window_label = f"{window_start.strftime('%H:%M')}â€“{window_end.strftime('%H:%M')}ï¼ˆè·¨åˆå¤œï¼‰"

    window_sales = float(cat_df.loc[in_window, "é”€é‡è´¡çŒ®"].sum())
    other_sales = max(range_total - window_sales, 0.0)
    share = window_sales / range_total if range_total > 0 else 0.0

    kA, kB, kC = st.columns(3)
    kA.metric("æŸ¥è¯¢èŒƒå›´æ€»é”€é‡", f"{int(range_total):,}")
    kB.metric(f"çª—å£å†…é”€é‡ï¼ˆ{window_label}ï¼‰", f"{int(window_sales):,}")
    kC.metric("çª—å£å æ¯”", f"{share*100:.2f}%")

    pie = pd.DataFrame({
        "éƒ¨åˆ†": [f"çª—å£å†… {window_label}", "çª—å£å¤–"],
        "é”€é‡": [window_sales, other_sales]
    })
    st.plotly_chart(px.pie(pie, names="éƒ¨åˆ†", values="é”€é‡", hole=0.45), use_container_width=True)

    with st.expander("æŸ¥çœ‹æŒ‰æ—¥æœŸæ‹†åˆ†ï¼ˆçª—å£å†…/çª—å£å¤–ï¼‰", expanded=False):
        tmp = cat_df.copy()
        tmp["æ—¥æœŸ"] = pd.to_datetime(tmp["æ—¶é—´æ®µ"]).dt.date.astype(str)
        tmp["æ˜¯å¦çª—å£å†…"] = in_window
        daily = (
            tmp.groupby(["æ—¥æœŸ", "æ˜¯å¦çª—å£å†…"], as_index=False)["é”€é‡è´¡çŒ®"].sum()
            .rename(columns={"é”€é‡è´¡çŒ®": "é”€é‡"})
        )
        daily["éƒ¨åˆ†"] = daily["æ˜¯å¦çª—å£å†…"].map({True: "çª—å£å†…", False: "çª—å£å¤–"})
        daily = daily.pivot(index="æ—¥æœŸ", columns="éƒ¨åˆ†", values="é”€é‡").fillna(0).reset_index()
        if "çª—å£å†…" not in daily.columns:
            daily["çª—å£å†…"] = 0
        if "çª—å£å¤–" not in daily.columns:
            daily["çª—å£å¤–"] = 0
        daily["æ€»è®¡"] = daily["çª—å£å†…"] + daily["çª—å£å¤–"]
        daily["çª—å£å æ¯”"] = daily["çª—å£å†…"] / daily["æ€»è®¡"].replace(0, np.nan)
        st.dataframe(daily, use_container_width=True)

# å…¶ä½™å¯è§†åŒ–ï¼šè¥ä¸šé¢ Top20 + æ—¶æ®µåˆ†ç±»å †å 
c1, c2 = st.columns(2)
with c1:
    st.subheader("å„åˆ†ç±»è¥ä¸šé¢ï¼ˆTop20ï¼Œå«å•åŠ -ï¼‰")
    st.plotly_chart(px.bar(rev_cat.head(20), x="è¥ä¸šé¢", y="åˆ†ç±»", orientation="h"), use_container_width=True)

with c2:
    st.subheader(f"å„æ—¶é—´æ®µå„åˆ†ç±»é”€é‡ï¼ˆ{grain}ï¼Œå«å•åŠ -ï¼‰")
    pivot = qty_time.pivot_table(index="æ—¶é—´æ®µ", columns="åˆ†ç±»", values="é”€é‡", aggfunc="sum").fillna(0).reset_index()
    y_cols = [c for c in pivot.columns if c != "æ—¶é—´æ®µ"]
    st.plotly_chart(px.bar(pivot, x="æ—¶é—´æ®µ", y=y_cols), use_container_width=True)

st.subheader("å„åˆ†ç±»æ€»é”€é‡ï¼ˆTop20ï¼Œå«å•åŠ -ï¼‰")
st.plotly_chart(px.bar(qty_cat_total.head(20), x="æ€»é”€é‡", y="åˆ†ç±»", orientation="h"), use_container_width=True)

st.subheader("å„åˆ†ç±»æ€»é”€é‡å æ¯”ï¼ˆTop20 + å…¶ä»–ï¼‰")
pie_df = make_topn_with_others(qty_cat_total[["åˆ†ç±»", "æ€»é”€é‡"]], "åˆ†ç±»", "æ€»é”€é‡", topn=20)
st.plotly_chart(px.pie(pie_df, names="åˆ†ç±»", values="æ€»é”€é‡", hole=0.45), use_container_width=True)

# =========================
# è§„æ ¼æ€»è®¡é”€é‡
# =========================
st.subheader("è§„æ ¼æ€»è®¡é”€é‡ï¼ˆè·¨åˆ†ç±»ï¼šå®½é¢/ç»†é¢/èœå“æ ‡å‡†/å…¶ä»–æ ‡å‡†ç­‰ï¼‰")
spec_col1, spec_col2, spec_col3 = st.columns([2, 1, 2])
with spec_col1:
    spec_topn = st.selectbox("æ˜¾ç¤º TopN", [10, 20, 50, 100, 999999], index=1)
with spec_col2:
    only_nonzero = st.checkbox("ä»…æ˜¾ç¤ºæœ‰é”€é‡", value=True)
with spec_col3:
    spec_search = st.text_input("è§„æ ¼æœç´¢ï¼ˆå…³é”®å­—ï¼‰", value="", placeholder="ä¾‹å¦‚ï¼šç±³é¥­ / å®½é¢ / ç»†é¢ / èœå“æ ‡å‡†")

spec_total = spec_total_full.copy()
if only_nonzero:
    spec_total = spec_total[spec_total["æ€»é”€é‡"] > 0]
if spec_search.strip():
    kw = spec_search.strip()
    spec_total = spec_total[spec_total["è§„æ ¼åç§°"].astype(str).str.contains(re.escape(kw), na=False)]
if spec_topn != 999999:
    spec_total = spec_total.head(int(spec_topn))
st.dataframe(spec_total, use_container_width=True)

# =========================
# æ˜ç»†è¡¨
# =========================
st.subheader("å„åˆ†ç±»æ€»é”€é‡ï¼ˆä¸åˆ†è§„æ ¼ï¼‰æ˜ç»†")
st.dataframe(qty_cat_total, use_container_width=True)

st.subheader("å„æ—¶é—´æ®µå„åˆ†ç±»é”€é‡ä¸å æ¯”ï¼ˆæ˜ç»†ï¼Œå«å•åŠ -ï¼‰")
st.dataframe(qty_time.sort_values(["æ—¶é—´æ®µ", "é”€é‡"], ascending=[True, False]), use_container_width=True)

st.subheader("å„åˆ†ç±»å„è§„æ ¼é”€é‡ä¸å æ¯”ï¼ˆå«å•åŠ -ï¼›ä¸ä¼šæ··å…¥å¤§ç±»ï¼‰")
st.dataframe(qty_spec.sort_values(["åˆ†ç±»", "é”€é‡"], ascending=[True, False]), use_container_width=True)

st.subheader("åšæ³•å•åŠ é¡¹ç»Ÿè®¡ï¼ˆå‡ºç°æ¬¡æ•° Ã— èœå“æ•°é‡ï¼‰")
st.dataframe(addon_summary, use_container_width=True)

st.subheader("æ‰“åŒ…ç›’ç»Ÿè®¡ï¼ˆå«åšæ³•â€œæ‰“åŒ…â€æ•°é‡ï¼‰")
st.dataframe(packaging_df, use_container_width=True)

# =========================
# 5) è§„åˆ™ä½“æ£€
# =========================
st.divider()
st.header("5) è§„åˆ™ä½“æ£€")

hc_col1, hc_col2 = st.columns([1, 3])
with hc_col1:
    do_health = st.button("ğŸ§ª è¿è¡Œè§„åˆ™ä½“æ£€", type="secondary")
with hc_col2:
    st.caption("è¾“å‡ºï¼šæœªå‘½ä¸­Topã€å¤šåˆ†ç±»å‘½ä¸­Topï¼ˆå†²çªï¼‰ã€å…³é”®è¯è¿‡å®½Topï¼ˆå¯èƒ½éœ€è¦æ”¶ç¼©/ç²¾ç¡®åŒ–ï¼‰")

if do_health:
    with st.spinner("ä½“æ£€ä¸­..."):
        unmatched_top, multi_hit_top, kw_wide = rule_health_check(df_q, CATEGORY_MAP, unmatched_df, cat_df)

    st.subheader("æœªå‘½ä¸­Topï¼ˆå»ºè®®è¡¥å……å…³é”®è¯ï¼‰")
    st.dataframe(unmatched_top, use_container_width=True)

    st.subheader("å¤šåˆ†ç±»å‘½ä¸­Topï¼ˆå¯èƒ½å…³é”®è¯è¿‡å®½/å†²çªï¼‰")
    st.dataframe(multi_hit_top, use_container_width=True)

    st.subheader("å…³é”®è¯è¿‡å®½Topï¼ˆå‘½ä¸­èœå“æ•°å¤šçš„å…³é”®è¯ï¼‰")
    st.dataframe(kw_wide, use_container_width=True)

# =========================
# 6) æŠ½æ ·å®¡è®¡
# =========================
st.divider()
st.header("6) æŠ½æ ·å®¡è®¡")

audit_col1, audit_col2, audit_col3 = st.columns([1, 1, 2])
with audit_col1:
    audit_n = st.selectbox("æŠ½æ ·è¡Œæ•°", [10, 20, 50, 100], index=1)
with audit_col2:
    audit_seed = st.number_input("éšæœºç§å­", min_value=0, max_value=999999, value=42, step=1)
with audit_col3:
    st.caption("éšæœºæŠ½åŸå§‹è¡Œ â†’ å±•ç¤ºå‘½ä¸­å“ªäº›åˆ†ç±»/æ˜¯å¦å•åŠ /æ˜¯å¦è›‹/æ—¶é—´æ¡¶ï¼Œæ–¹ä¾¿å¿«é€Ÿæ ¸å¯¹è§„åˆ™ä¸å£å¾„ã€‚")

rng = np.random.default_rng(int(audit_seed))
if len(df_q) > 0:
    sample_idx = rng.choice(df_q.index.to_numpy(), size=min(int(audit_n), len(df_q)), replace=False)
    audit_raw = df_q.loc[sample_idx, ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "èœå“æ•°é‡", "è§„æ ¼åç§°", "åšæ³•", "ä¼˜æƒ åå°è®¡ä»·æ ¼", "æ—¶é—´æ®µ"]].copy()

    key_cols = ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "èœå“æ•°é‡", "è§„æ ¼åç§°", "åšæ³•", "ä¼˜æƒ åå°è®¡ä»·æ ¼", "æ—¶é—´æ®µ"]
    audit_raw["_k"] = audit_raw[key_cols].astype(str).agg("|".join, axis=1)
    tmp = cat_df.copy()
    tmp["_k"] = tmp[key_cols].astype(str).agg("|".join, axis=1)

    hit = (
        tmp.groupby("_k", as_index=False)
        .agg(
            å‘½ä¸­åˆ†ç±»=("åˆ†ç±»åŸºç¡€", lambda x: "ï¼Œ".join(sorted(set(map(str, x))))),
            å±•ç¤ºåˆ†ç±»=("åˆ†ç±»", lambda x: "ï¼Œ".join(sorted(set(map(str, x))))),
        )
    )

    audit = audit_raw.merge(hit, on="_k", how="left")
    audit["æ˜¯å¦å•åŠ "] = audit["èœå“åç§°"].astype(str).str.strip().str.startswith("åŠ ")
    audit["æ˜¯å¦è›‹ç±»"] = audit["å‘½ä¸­åˆ†ç±»"].astype(str).str.contains("å¤è›‹ç±»|ç…è›‹ç±»", na=False)
    audit = audit.drop(columns=["_k"])
    st.dataframe(audit, use_container_width=True)
else:
    st.info("å½“å‰æ—¶é—´èŒƒå›´å†…æ— æ•°æ®ï¼Œæ— æ³•æŠ½æ ·å®¡è®¡ã€‚")

# =========================
# 7) åˆ†ç±»è´¨é‡æ£€æŸ¥ï¼ˆè¦†ç›–ç‡ + æœªå‘½ä¸­ï¼‰
# =========================
st.divider()
st.header("7) åˆ†ç±»è´¨é‡æ£€æŸ¥")

st.subheader("åˆ†ç±»è¦†ç›–ç‡")
st.dataframe(coverage_df, use_container_width=True)

st.subheader("æœªå‘½ä¸­åˆ†ç±»çš„èœå“ï¼ˆå»ºè®®è¡¥å……å…³é”®è¯ï¼‰")
unmatched_agg = (
    unmatched_df.groupby("èœå“åç§°", as_index=False)
    .agg(
        æœªå‘½ä¸­è¡Œæ•°=("èœå“åç§°", "size"),
        æ•°é‡åˆè®¡=("èœå“æ•°é‡", "sum"),
        å°è®¡åˆè®¡=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum")
    )
    .sort_values(["æ•°é‡åˆè®¡", "å°è®¡åˆè®¡"], ascending=False)
)
st.dataframe(unmatched_agg, use_container_width=True)

# =========================
# 8) å¯¼å‡º Excelï¼ˆå¤š sheetï¼‰
# =========================
@st.cache_data(show_spinner=False)
def export_excel(
    qty_time, qty_cat_total, qty_spec, spec_total_full, rev_cat,
    addon_summary, packaging_df, coverage_df, unmatched_agg,
    unmatched_df, rule_fingerprint, rule_source,
    window_start: dt.time, window_end: dt.time
) -> bytes:
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        meta = pd.DataFrame([
            {"é”®": "è§„åˆ™æ¥æº", "å€¼": rule_source},
            {"é”®": "è§„åˆ™æŒ‡çº¹", "å€¼": rule_fingerprint},
            {"é”®": "å¯¼å‡ºæ—¶é—´", "å€¼": dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")},
            {"é”®": "çª—å£å¼€å§‹(ä¸€å¤©å†…)", "å€¼": window_start.strftime("%H:%M")},
            {"é”®": "çª—å£ç»“æŸ(ä¸€å¤©å†…)", "å€¼": window_end.strftime("%H:%M")},
        ])
        meta.to_excel(writer, sheet_name="å…ƒä¿¡æ¯", index=False)

        qty_cat_total.to_excel(writer, sheet_name="åˆ†ç±»æ€»é”€é‡", index=False)
        qty_time.to_excel(writer, sheet_name="æ—¶æ®µ_åˆ†ç±»é”€é‡å æ¯”", index=False)
        qty_spec.to_excel(writer, sheet_name="åˆ†ç±»_è§„æ ¼é”€é‡å æ¯”", index=False)
        spec_total_full.to_excel(writer, sheet_name="è§„æ ¼æ€»é”€é‡_å…¨é‡", index=False)
        rev_cat.to_excel(writer, sheet_name="åˆ†ç±»è¥ä¸šé¢", index=False)
        addon_summary.to_excel(writer, sheet_name="åšæ³•å•åŠ é¡¹ç»Ÿè®¡", index=False)
        packaging_df.to_excel(writer, sheet_name="æ‰“åŒ…ç›’ç»Ÿè®¡", index=False)
        coverage_df.to_excel(writer, sheet_name="åˆ†ç±»è¦†ç›–ç‡", index=False)
        unmatched_agg.to_excel(writer, sheet_name="æœªå‘½ä¸­èœå“æ±‡æ€»", index=False)
        unmatched_df.to_excel(writer, sheet_name="æœªå‘½ä¸­æ˜ç»†", index=False)
    return output.getvalue()

st.divider()
st.header("8) å¯¼å‡ºç»“æœ")

xlsx_bytes = export_excel(
    qty_time, qty_cat_total, qty_spec, spec_total_full, rev_cat,
    addon_summary, packaging_df, coverage_df, unmatched_agg,
    unmatched_df, rule_fingerprint, rule_source,
    window_start, window_end
)
st.download_button(
    label="ä¸‹è½½ç»Ÿè®¡ç»“æœï¼ˆExcelï¼‰",
    data=xlsx_bytes,
    file_name="èœå“é”€å”®åˆ†æ.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)
